---
layout: post
title: SQL Data pipelines
subtitle: A practical guide of setting up a SQL data pipeline with dbt and gitlab
  CI
date: 2020-07-30 14:45:13 +0000
background: "/img/posts/06.jpg"

---
<blockquote class="blockquote">Every day, three times per second, we produce the equivalent of the amount of data that the Library of Congress has in its entire print collection, right? But most of it is like cat videos on YouTube or 13-year-olds exchanging text messages about the next Twilight movie.â€“ Nate Silver</blockquote>

<h2 class="section-heading">Data pipelines</h2>
<p>For data driven organizations, one of the most critical operations is the efficient flow of data from source systems to target systems.Useful analysis can only begin once data is availed to it's end users in a formart that is easy to use and is accessible</p>

<p>Moving data from one place to another, especially data of large volumes can be a daunting task. This is because data can be corrupted, data can be coming in slow i.e. hitting latency bottlenecks or data sources/desitinations may conflict</p>

<p>With this problems in mind, there has been need of creating softwares that eliminate the manual steps from the data flow process. This is by automating extraction, loading ,transformation,testing and documentation of data for further analysis. The softwares make up a data pipeline that readies data for the end users</p>

<p> Data pipelines can handle the following types of processing.</p>
<ul>
<li>Streaming processing - this involves processing data almost 			 instantaneously as it streams from source to target destinationg e.g. IOT data</li>
 <li>Batch processing - this is the processing of large volumes of data at 	once over specified intervals of time</li>
</ul>

<p>This blog will be focus on a theoritical introduction on how to set up a simple batch processing data pipeline with SQL. The following blog posts as listed below will focus on a practical example of setting this up using the tools in this blog.</p>

<ul>
  <li>Part 2: Setting up dbt(data build tool), Postgresql and gitlab ci</li>
  <li>Part 3: Data modelling, documentation and testing</li>
  <li>Part 4: Version controll and continous integration</li>
  <li>Part 5: Visualization with tableau</li>
</ul>

<h2 class="section-heading">The problem</h2>

<blockquote class="blockquote">Take an example of a company that primarily depends on SQL for data analysis. Since the company started, it has been using an open source BI tool that allows analysts to write adhoc queries and visualize quickly.As the company scales, so does the team grow and soon they may run into the following problems:
</blockquote>
 <ul>
   <li>Analysts have to write queries for each new problem</li>
   <li>There is no testing of the SQL queries</li>
   <li>It is hard to tell which queries are used where</li>
   <li>A single change can cause some dependent queries to fail</li>
   <li>Many undefined metrics and dimensions</li>
   <li>The SQL queries have to be manually updated</li>
  </ul>

<h2 class="section-heading">The solution</h2>

<p>The above is a typical growing pain of most startups. The initial team is usually consisted of just one person who builds all the SQL models and visualizations. As the team grows, it becomes hard to know which models changed or who changed what or what changed where. This can cause significant issues with correctness and reliability of data.</p>

<p>Building an SQL pipeline will be able to sort out some of these growing pains. An SQL pipeline will allow the team to build reusable datasets and focus on generating insights instead of copy pasting old SQL and build same queries over and over again.</p>

<h3>A simple SQL pipeline</h3>

<p>There are many tools in the market, howevere in this blog we will talk about how <code>dbt(data build tool)</code> in combination with <code>gitlab ci</code> can be used to build a simple SQL pipeline to solve the above problem</p>

<h5>DBT(data build tool)</h5>
<p>dbt or data build tool is an open source command line tool that enables analysts to do the data transformation work that happens after data is loaded to the data warehouse.</p>

<p>It sits at the end of the 'ELT' stack, i.e. transformation which is carried out within the database.It takes code and compiles it to SQL then runs this against the datawarehouse.The SQL code is stored in a <code>.sql</code> file called a model. These models are single <code>SELECT...</code> statements that define the resulting data set after compilation</p>

<p>Using principles of software engineering, dbt enable analysts:</p>
<ul>
  <li>Document SQL code - This is stored in the form of a static <code>HTML</code> site that can be viewed by users of the data</li>
  <li>Version control SQL models - since SQL statements are stored in files, version control can be applied to manage change of the SQL statements</li>

<h5>Gitlab CI</h5>


<p>Toptal skill reference:<a href="https://www.toptal.com/data-modeling-analysts">Data modeling analyst</a></p>

<p>Toptal skill reference:<a href="https://www.toptal.com/data-engineer">Data engineer</a></p>