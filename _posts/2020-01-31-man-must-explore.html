---
layout: post
title: SQL Data pipelines
subtitle: A practical guide of setting up a SQL data pipeline with dbt and gitlab
  CI
date: 2020-04-24 14:45:13 +0000
background: "/img/posts/06.jpg"

---
<blockquote class="blockquote">Every day, three times per second, we produce the equivalent of the amount of data that the Library of Congress has in its entire print collection, right? But most of it is like cat videos on YouTube or 13-year-olds exchanging text messages about the next Twilight movie.â€“ Nate Silver</blockquote>

<h2 class="section-heading">Data pipelines</h2>
<p>For data driven organizations, one of the most critical operations is the efficient flow of data from source systems to target systems.Useful analysis can only begin once data is availed to it's end users in a formart that is easy to use and is accessible</p>

<p>Moving data from one place to another, especially data of large volumes can be a daunting task. This is because data can be corrupted, data can be coming in slow i.e. hitting latency bottlenecks or data sources/desitinations may conflict</p>

<p>With this problems in mind, there has been need of creating softwares that eliminate the manual steps from the data flow process. This is by automating extraction, loading ,transformation,testing and documentation of data for further analysis. The softwares make up a data pipeline that readies data for the end users</p>

<p> Data pipelines can handle the following types of processing.</p>
<ul>
<li>Streaming processing - this involves processing data almost 			 instantaneously as it streams from source to target destinationg e.g. IOT data</li>
 <li>Batch processing - this is the processing of large volumes of data at 	once over specified intervals of time</li>
</ul>

<p>This blog will be focus on a theoritical introduction on how to set up a simple batch processing data pipeline with SQL. The following blog posts as listed below will focus on a practical example of setting this up using the tools in this blog.</p>

<ul>
  <li>Part 2: Setting up dbt(data build tool), Postgresql and gitlab ci</li>
  <li>Part 3: Data modelling, documentation and testing</li>
  <li>Part 4: Version controll and continous integration</li>
  <li>Part 5: Visualization with tableau</li>
</ul>

<h2 class="section-heading">The problem</h2>

<blockquote class="blockquote">You are an analystics engineer in a company that primarily depends on SQL for data analysis. They have been using a BI tool that allows them to write adhoc queries and visualize quickly. The data team is scaling and have recently run into the following problems:<br> 
 <ul>
   <li>Analysts have to write queries for each new problem</li>
   <li>There is no testing of the SQL queries</li>
   <li>It is hard to tell which queries are used where</li>
   <li>Queries fail downstream when a simple change happens to one model,causing a maintenance nightmare</li>
   <li>No clear definition of what some of the columns in then models mean</li>
   <li>The SQL queries have to be manually updated</li>
  </ul>
</blockquote>




<p>Toptal skill reference:<a href="https://www.toptal.com/data-modeling-analysts">Data modeling analyst</a></p>

<p>Toptal skill reference:<a href="https://www.toptal.com/data-engineer">Data engineer</a></p>